{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior of token-level parameters\n",
    "\n",
    "This notebook computes the sufficient statistics for token-level parameters from (partially) matched name lists. Since all token parameters are independent given matches and global prior parameters, this can be done for each token individually.\n",
    "\n",
    "The likelihood can be computed directly from the Beta-Binomial pmf,\n",
    "\n",
    "$$\n",
    "p(k \\mid n, \\alpha, \\beta) = \\frac{Z(\\alpha + k, \\beta + n - k)}{Z(\\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Z(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n",
    "$$\n",
    "\n",
    "is the normalizer of the Beta-distribution.\n",
    "\n",
    "We use a mean-strength parametrization\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha &= s \\pi \\\\\n",
    "\\beta &= s (1 - \\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\pi \\in [0, 1]$ is an the prior probability of an unseen entity to contain the token in its name, so that it uses entities as the denominator (not names). Consequently, the estimate changes when names are matched to entities. It is also weakly correlated with $s$. This tends to be relatively well determined by the data except for very rare tokens.\n",
    "\n",
    "$s > 0$ measures how consistently a token is used: For $s \\rightarrow 0$ we expect a token to occur with certainty in all names of an entity if it has occured in one name. For $s \\rightarrow \\infty$ the frequency is the population frequency regardless of whether the token was part of a name of an entity or not. Hence, low values of $s$ correspond to more relevant tokens, while tokens with large values are effectively ignored.\n",
    "\n",
    "We impose a log-normal prior on $s$ and a flat prior on $\\log\\pi$, since $\\pi$ is much better determined by the data, while for $s$ the likelihood is not even a normalizable distribution in some cases.\n",
    "\n",
    "\n",
    "In principle, the likelihood contains one factor per entity, but in practice there are large multiplicities. For pairwise matches, the only possible values of the sufficient statistics are $n = 1, k = 0,1$ and $n=2, k=0,1,2$. We compute the likelihood for the distinct sufficient statistics only once while taking multiplicities into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from ptfidf import utils as ut\n",
    "from ptfidf.train.aggregation import get_group_statistics, compress_group_statistics\n",
    "from ptfidf.train.likelihood import beta_binomial_log_likelihood\n",
    "from ptfidf.train.inference import map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_training_data(names, target, matched_symbols):\n",
    "    \"\"\"\n",
    "    Label training data.\n",
    "    \n",
    "    Names with symbol in matched_symbols will be assigned to \n",
    "    the same entity (per symbol).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    names : pandas.DataFrame\n",
    "        columns ['source', 'symbol', ...]\n",
    "    target : str\n",
    "        value of source that is to be considered target list\n",
    "    matched_symbols : sequence or int\n",
    "        if sequence, these symbols are matched. If int,\n",
    "        draw matched_symbols symbols randomly.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        values are '{source}:{symbol}' where source equals target for\n",
    "        matched symbols.\n",
    "    \"\"\"\n",
    "    if isinstance(matched_symbols, int):\n",
    "        matched_symbols = np.random.choice(np.unique(names['symbol']), replace=False, size=matched_symbols)\n",
    "    res = pd.Series(index=names.index, name='entity')\n",
    "    idx = names['symbol'].isin(matched_symbols)\n",
    "    res.loc[idx] = target + ':' + names.loc[idx, 'symbol']\n",
    "    res.loc[~idx] = names.loc[~idx, 'source'] + ':' + names.loc[~idx, 'symbol']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Read name files and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files and concatenate into single table\n",
    "datadir = Path('../data').resolve()\n",
    "filenames = ['wikipedia.csv', 'slickcharts.csv']\n",
    "names = pd.concat([\n",
    "    pd.read_csv(datadir.joinpath(fn)).assign(source=fn.split('.')[0]) for fn in filenames\n",
    "], axis=0).sort_values(['symbol', 'source']).reset_index(drop=True)\n",
    "\n",
    "# drop some symbols that are impossible to get correct with current preprocessing\n",
    "\n",
    "# These differ only in appended Class <X>, where <X> is a single letter that is \n",
    "# dropped in the preprocessing\n",
    "duplicate_companies = ['GOOGL', 'NWSA', 'DISCA', 'FOXA', 'UAA']\n",
    "# No (or almost no) token overlap due to spelling variations\n",
    "impossible_symbols = ['VFC', 'PHM', 'UNH', 'LLL']\n",
    "names = names[~names['symbol'].isin(duplicate_companies + impossible_symbols)]\n",
    "\n",
    "# vectorize\n",
    "vec = CountVectorizer(binary=True).fit(names['name'])\n",
    "X = vec.transform(names['name'])\n",
    "vocabulary_inv = {v: k for k, v in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label training data\n",
    "\n",
    "Since there are many tokens with the same statistics (e.g. tokens that occur only once or twice), also compute distinct token statistics for more convenient display of interesting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_training_data(names, 'wikipedia', 400)\n",
    "y = LabelEncoder().fit_transform(labels.values)\n",
    "counts, nobs = get_group_statistics(X, y)\n",
    "n, k, weights = compress_group_statistics(counts, nobs)\n",
    "\n",
    "# get distinct token statistics and associated tokens\n",
    "distinct_weights, distinct_idx = np.unique(weights, axis=0, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A look at the posterior\n",
    "\n",
    "Show joint distribution of token-level parameters for selected token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "prior_mean = np.log(.2)\n",
    "prior_std = 2.\n",
    "max_token_display = 5\n",
    "\n",
    "# grid\n",
    "pivals = np.linspace(np.log(1e-5), np.log(.5), 100)\n",
    "svals = np.linspace(np.log(1e-4), np.log(1e3), 100)\n",
    "s, pi = np.meshgrid(svals, pivals)\n",
    "alpha, beta = np.exp(s) * np.exp(pi), np.exp(s) * (1. - np.exp(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(distinct_weights.shape[0])\n",
    "\n",
    "selected_tokens = np.where(distinct_idx == i)[0]\n",
    "\n",
    "output = ', '.join(vocabulary_inv[t] for t in selected_tokens[:max_token_display])\n",
    "if len(selected_tokens) > max_token_display:\n",
    "    output += ', ... ({} more)'.format(len(selected_tokens) - max_token_display)\n",
    "print('tokens:\\n' + output)\n",
    "\n",
    "\n",
    "post = beta_binomial_log_likelihood(alpha[..., None], beta[..., None], k, n).dot(distinct_weights[i])\n",
    "post -= .5 * (s - prior_mean)**2 / prior_std**2\n",
    "post = np.exp(post - post.max())\n",
    "post /= post.sum()\n",
    "\n",
    "pi_opt, s_opt = map_estimate(n, k, distinct_weights[i:i+1], prior_mean, prior_std)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "\n",
    "\n",
    "lg10 = np.log(10)\n",
    "ax = axes[0]\n",
    "ax.set_xlabel('$log_{10} s$')\n",
    "ax.set_ylabel('$log_{10} \\pi$')\n",
    "ax.pcolormesh(s / lg10, pi / lg10, post, cmap=plt.cm.Blues)\n",
    "ax.plot([np.log10(s_opt)], [np.log10(pi_opt)], 'ro')\n",
    "ax.grid()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title('marginal distribution $\\log_{10}s$')\n",
    "ax.plot(svals / lg10, post.sum(axis=0))\n",
    "ax.axvline(np.log10(s_opt), color='r')\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title('marginal distribution $\\log_{10}\\pi$')\n",
    "ax.plot(pivals / lg10, post.sum(axis=1))\n",
    "ax.axvline(np.log10(pi_opt), color='r')\n",
    "\n",
    "pd.DataFrame({'n': n, 'k': k, 'weights': distinct_weights[i]}).set_index(['n', 'k'])['weights']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
