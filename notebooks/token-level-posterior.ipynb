{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior of token-level parameters\n",
    "\n",
    "This notebook computes the sufficient statistics for token-level parameters from (partially) matched name lists. Since all token parameters are independent given matches and global prior parameters, this can be done for each token individually.\n",
    "\n",
    "The likelihood can be computed directly from the Beta-Binomial pmf,\n",
    "\n",
    "$$\n",
    "p(k \\mid n, \\alpha, \\beta) = \\frac{Z(\\alpha + k, \\beta + n - k)}{Z(\\alpha, \\beta)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Z(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n",
    "$$\n",
    "\n",
    "is the normalizer of the Beta-distribution.\n",
    "\n",
    "We use a mean-strength parametrization\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha &= s \\pi \\\\\n",
    "\\beta &= s (1 - \\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\pi \\in [0, 1]$ is an the prior probability of an unseen entity to contain the token in its name, so that it uses entities as the denominator (not names). Consequently, the estimate changes when names are matched to entities. It is also weakly correlated with $s$. This tends to be relatively well determined by the data except for very rare tokens.\n",
    "\n",
    "$s > 0$ measures how consistently a token is used: For $s \\rightarrow 0$ we expect a token to occur with certainty in all names of an entity if it has occured in one name. For $s \\rightarrow \\infty$ the frequency is the population frequency regardless of whether the token was part of a name of an entity or not. Hence, low values of $s$ correspond to more relevant tokens, while tokens with large values are effectively ignored.\n",
    "\n",
    "We impose a log-normal prior on $s$ and a flat prior on $\\log\\pi$, since $\\pi$ is much better determined by the data, while for $s$ the likelihood is not even a normalizable distribution in some cases.\n",
    "\n",
    "\n",
    "In principle, the likelihood contains one factor per entity, but in practice there are large multiplicities. For pairwise matches, the only possible values of the sufficient statistics are $n = 1, k = 0,1$ and $n=2, k=0,1,2$. We compute the likelihood for the distinct sufficient statistics only once while taking multiplicities into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from ptfidf import utils as ut\n",
    "from ptfidf.train.aggregation import get_group_statistics, compress_group_statistics\n",
    "from ptfidf.train.likelihood import beta_binomial_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_training_data(names, matched_symbols):\n",
    "    \"\"\"label training data.\n",
    "    \n",
    "    Names with symbol in matched_symbols will be assigned to \n",
    "    the same entity (per symbol).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    names : DataFrame\n",
    "        columns ['source', 'symbol', ...]\n",
    "    matched_symbols : sequence or int\n",
    "        if sequence, these symbols are matched. If int,\n",
    "        draw matched_symbols symbols randomly.\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        with added column 'entity', which is '{symbol}' or\n",
    "        '{source}:{symbol}' for matched respectively unmatched\n",
    "        symbols.\n",
    "    \"\"\"\n",
    "    if isinstance(matched_symbols, int):\n",
    "        matched_symbols = np.random.choice(np.unique(names['symbol']), replace=False, size=matched_symbols)\n",
    "    res = pd.Series(index=names.index, name='entity')\n",
    "    idx = names['symbol'].isin(matched_symbols)\n",
    "    res.loc[idx] = names.loc[idx, 'symbol']\n",
    "    res.loc[~idx] = names.loc[~idx, 'source'] + ':' + names.loc[~idx, 'symbol']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Read name files and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files and concatenate into single table\n",
    "datadir = Path('../data').resolve()\n",
    "filenames = ['wikipedia.csv', 'slickcharts.csv']\n",
    "names = pd.concat([\n",
    "    pd.read_csv(datadir.joinpath(fn)).assign(source=fn.split('.')[0]) for fn in filenames\n",
    "], axis=0).sort_values(['symbol', 'source']).reset_index(drop=True)\n",
    "\n",
    "# vectorize\n",
    "vec = CountVectorizer(binary=True).fit(names['name'])\n",
    "X = vec.transform(names['name'])\n",
    "vocabulary_inv = {v: k for k, v in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label training data\n",
    "\n",
    "Since there are many tokens with the same statistics (e.g. tokens that occur only once or twice), also compute distinct token statistics for more convenient display of interesting examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_training_data(names, np.unique(names['symbol']))\n",
    "y = LabelEncoder().fit_transform(labels.values)\n",
    "counts, nobs = get_group_statistics(X, y)\n",
    "token_stats = compress_group_statistics(counts, nobs)\n",
    "\n",
    "# get distinct token statistics and associated tokens\n",
    "distinct = (\n",
    "    token_stats\n",
    "    .groupby('token')\n",
    "    .apply(lambda df: tuple(df[['n', 'k', 'weight']].apply(tuple, axis=1).sort_values()))\n",
    "    .reset_index()\n",
    "    .groupby(0)['token']\n",
    "    .agg(lambda x: tuple(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Visualization\n",
    "\n",
    "Show joint distribution of token-level parameters for selected token\n",
    "\n",
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = np.log(.2)\n",
    "prior_std = 2.\n",
    "max_token_display = 5\n",
    "\n",
    "\n",
    "pivals = np.linspace(np.log(1e-5), np.log(.5), 200)\n",
    "svals = np.linspace(np.log(1e-4), np.log(1e3), 200)\n",
    "s, pi = np.meshgrid(svals, pivals)\n",
    "alpha, beta = np.exp(s) * np.exp(pi), np.exp(s) * (1. - np.exp(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior computation and visualization\n",
    "\n",
    "The posterior is computed on a grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(distinct.size)\n",
    "selected_tokens = distinct.iloc[i]\n",
    "\n",
    "output = ', '.join(vocabulary_inv[t] for t in selected_tokens[:max_token_display])\n",
    "if len(selected_tokens) > max_token_display:\n",
    "    output += ', ... ({} more)'.format(len(selected_tokens) - max_token_display)\n",
    "print('tokens:\\n' + output)\n",
    "\n",
    "ms = token_stats[token_stats['token'] == selected_tokens[0]].sort_values(['n', 'k'])[['k', 'n', 'weight']].copy()\n",
    "\n",
    "res = beta_binomial_log_likelihood(\n",
    "    alpha[..., None], beta[..., None],\n",
    "    ms['k'].values, ms['n'].values).dot(ms['weight'].values\n",
    ")\n",
    "res -= .5 * (s - prior_mean)**2 / prior_std**2\n",
    "res = np.exp(res - res.max())\n",
    "res /= res.sum()\n",
    "\n",
    "idx_smp = np.random.choice(res.size, p=res.ravel(), size=1000)\n",
    "s_smp, pi_smp = [np.exp(arr.ravel()[idx_smp]) for arr in [s, pi]]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "\n",
    "lg10 = np.log(10)\n",
    "ax = axes[0]\n",
    "ax.set_xlabel('$log_{10} s$')\n",
    "ax.set_ylabel('$log_{10} \\pi$')\n",
    "ax.pcolormesh(s / lg10, pi / lg10, res, cmap=plt.cm.Blues)\n",
    "ax.plot(np.log(s_smp[:20]) / lg10, np.log(pi_smp[:20]) / lg10, 'r.', alpha=.6)\n",
    "ax.grid()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title('marginal distribution $\\log_{10}s$')\n",
    "ax.plot(svals / lg10, res.sum(axis=0))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title('marginal distribution $\\log_{10}\\pi$')\n",
    "ax.plot(pivals / lg10, res.sum(axis=1))\n",
    "\n",
    "ms.set_index(['n', 'k'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
